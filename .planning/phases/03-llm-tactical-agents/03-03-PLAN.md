---
phase: 03-llm-tactical-agents
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - src/agents/llm_agent.py
  - src/cli/batch_args.py
  - run.py
autonomous: true
user_setup:
  - service: ollama
    why: "Local LLM inference for tactical decisions"
    env_vars: []
    dashboard_config:
      - task: "Install Ollama and pull a model"
        location: "Terminal: curl -fsSL https://ollama.com/install.sh | sh && ollama pull qwen2.5:7b-instruct"
  - service: openrouter
    why: "Cloud LLM inference as alternative to local Ollama"
    env_vars:
      - name: OPENROUTER_API_KEY
        source: "OpenRouter Dashboard -> Keys (https://openrouter.ai/keys)"
    dashboard_config: []

must_haves:
  truths:
    - "LLM agent calls provider, parses response, validates move, and returns AgentAction"
    - "On parse failure or illegal move, system falls back to HeuristicAgent seamlessly"
    - "After 3 consecutive LLM failures in a combat, circuit breaker disables LLM for rest of combat"
    - "CLI --agent llm flag activates LLM agent instead of heuristic"
    - "CLI --provider ollama/openrouter selects which LLM backend to use"
    - "CLI --model flag lets user specify model name"
  artifacts:
    - path: "src/agents/llm_agent.py"
      provides: "LLMAgent orchestration with circuit breaker and fallback"
      exports: ["LLMAgent"]
    - path: "src/cli/batch_args.py"
      provides: "Updated CLI args with --agent, --provider, --model flags"
      contains: "agent"
    - path: "run.py"
      provides: "Updated CLI entry point that wires LLM agent"
      contains: "LLMAgent"
  key_links:
    - from: "src/agents/llm_agent.py"
      to: "src/agents/llm_providers.py"
      via: "calls provider.chat_completion"
      pattern: "provider\\.chat_completion"
    - from: "src/agents/llm_agent.py"
      to: "src/agents/llm_prompt.py"
      via: "calls build_prompt to construct messages"
      pattern: "build_prompt"
    - from: "src/agents/llm_agent.py"
      to: "src/agents/llm_parser.py"
      via: "calls parse_llm_output on LLM text"
      pattern: "parse_llm_output"
    - from: "src/agents/llm_agent.py"
      to: "src/agents/llm_validator.py"
      via: "calls validate_move and to_agent_action"
      pattern: "validate_move|to_agent_action"
    - from: "src/agents/llm_agent.py"
      to: "src/agents/heuristic.py"
      via: "falls back to HeuristicAgent on failure"
      pattern: "HeuristicAgent"
    - from: "run.py"
      to: "src/agents/llm_agent.py"
      via: "instantiates LLMAgent when --agent llm"
      pattern: "LLMAgent"
---

<objective>
Create the LLMAgent orchestrator with circuit breaker + heuristic fallback, and wire it into the CLI.

Purpose: This is the integration layer that connects providers, prompt builder, parser, and validator into a working agent. The circuit breaker ensures batch simulations don't hang when the LLM is unavailable. CLI integration lets the user actually run simulations with LLM agents.

Output: `llm_agent.py` (LLMAgent with choose_action, circuit breaker, fallback), updated `batch_args.py` and `run.py` with --agent/--provider/--model flags.
</objective>

<execution_context>
@/Users/enrico/.claude/get-shit-done/workflows/execute-plan.md
@/Users/enrico/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-tactical-agents/03-RESEARCH.md

# These files are created by Plans 01 and 02 -- read their SUMMARYs for exact interfaces
@.planning/phases/03-llm-tactical-agents/03-01-SUMMARY.md
@.planning/phases/03-llm-tactical-agents/03-02-SUMMARY.md

@src/agents/base.py
@src/agents/heuristic.py
@src/cli/batch_args.py
@run.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLMAgent with circuit breaker and fallback</name>
  <files>src/agents/llm_agent.py</files>
  <action>
Create `src/agents/llm_agent.py` implementing the BaseAgent protocol (synchronous `choose_action`).

**LLMAgent class:**

```python
class LLMAgent:
    def __init__(
        self,
        provider: LLMProvider,
        model: str = "qwen2.5:7b-instruct",
        role: str = "default",
        max_retries: int = 2,
        circuit_breaker_threshold: int = 3,
    ):
```

Fields:
- `self.provider` - LLMProvider instance (Ollama or OpenRouter)
- `self.fallback` - HeuristicAgent() instance, created internally
- `self.model` - model name string
- `self.role` - creature role archetype for prompt
- `self.max_retries` - retries per call (for transient errors)
- `self.circuit_breaker_threshold` - consecutive failures before disabling LLM
- `self._consecutive_failures` - int counter, reset on success
- `self._circuit_open` - bool, True when LLM disabled for this combat

**choose_action(self, state, creature_id: str) -> AgentAction:**

1. **Circuit breaker check:** If `self._circuit_open`, immediately return `self.fallback.choose_action(state, creature_id)`.

2. **Death save shortcut:** If creature is at 0 HP, delegate to fallback (no need to call LLM for death saves).

3. **Build prompt:** Call `build_prompt(state, creature_id, self.role)` to get messages.

4. **Call LLM with retry:** Use tenacity `@retry` decorator on an internal method `_call_provider(messages)` that calls `self.provider.chat_completion(messages, self.model)`. Retry up to `self.max_retries` times on `ConnectionError`, `TimeoutError`, and `OSError` (Ollama connection refused). Use `wait_exponential(multiplier=1, min=1, max=5)`.

5. **Parse response:** Call `parse_llm_output(response_text)`. If ValueError, log warning and fall back.

6. **Validate move:** Call `validate_move(parsed, state, creature_id)`. If invalid, log the error message and fall back.

7. **Convert to action:** Call `to_agent_action(parsed, state, creature_id)`. Return the AgentAction.

8. **On any success:** Reset `self._consecutive_failures = 0`.

9. **On any failure** (parse error, validation error, provider error after retries):
   - Increment `self._consecutive_failures`
   - If `>= self.circuit_breaker_threshold`: set `self._circuit_open = True`, log "Circuit breaker opened: falling back to heuristic for remaining combat"
   - Return `self.fallback.choose_action(state, creature_id)`

**reset_circuit_breaker(self):**
Public method to reset circuit breaker between combats. Call this at the start of each new combat run.
- `self._consecutive_failures = 0`
- `self._circuit_open = False`

**Important:** Use Python `logging` module for all log messages (not print). Logger name: `dnd_sim.llm_agent`. Log at WARNING level for fallbacks, INFO for successful LLM calls, DEBUG for prompt/response content.

**Do NOT make choose_action async.** The combat simulator calls it synchronously. The providers are synchronous. Everything stays sync.
  </action>
  <verify>Run `cd /Users/enrico/workspace/myobsidian/dnd-simulator && python -c "
from src.agents.llm_agent import LLMAgent
from src.agents.llm_providers import OllamaProvider
agent = LLMAgent(provider=OllamaProvider(), model='qwen2.5:7b-instruct', role='striker')
print('LLMAgent created OK')
print('Has choose_action:', hasattr(agent, 'choose_action'))
print('Has reset_circuit_breaker:', hasattr(agent, 'reset_circuit_breaker'))
print('Fallback is HeuristicAgent:', type(agent.fallback).__name__)
"` -- LLMAgent instantiates correctly with expected interface.</verify>
  <done>LLMAgent implements choose_action with: prompt building, LLM call with retry, response parsing, move validation, circuit breaker (3 consecutive failures), and HeuristicAgent fallback. Circuit breaker can be reset between combats.</done>
</task>

<task type="auto">
  <name>Task 2: Wire LLM agent into CLI</name>
  <files>src/cli/batch_args.py, run.py</files>
  <action>
**Update `src/cli/batch_args.py`:**

Add three new arguments to the argparse parser:

1. `--agent` with choices=["heuristic", "llm"], default="heuristic". Help: "Agent type for action selection (heuristic or llm)"

2. `--provider` with choices=["ollama", "openrouter"], default="ollama". Help: "LLM provider (only used with --agent llm)"

3. `--model` with type=str, default=None. Help: "LLM model name (default: qwen2.5:7b-instruct for ollama, qwen/qwen2.5-coder-7b-instruct for openrouter)"

4. `--role` with choices=["default", "tank", "striker", "controller", "support"], default="default". Help: "Creature role archetype for LLM tactical behavior"

Add these 4 fields to the BatchArgs dataclass: `agent: str`, `provider: str`, `model: Optional[str]`, `role: str`.

Add validation: if `--agent llm` and `--provider openrouter`, check that `OPENROUTER_API_KEY` environment variable is set (or warn).

**Update `run.py`:**

In the `main()` function, replace the hardcoded `agent = HeuristicAgent()` with agent selection logic:

```python
if args.agent == "llm":
    from src.agents.llm_agent import LLMAgent
    from src.agents.llm_providers import OllamaProvider, OpenRouterProvider
    import os

    # Select provider
    if args.provider == "ollama":
        provider = OllamaProvider()
    elif args.provider == "openrouter":
        api_key = os.environ.get("OPENROUTER_API_KEY")
        if not api_key:
            print("Error: OPENROUTER_API_KEY environment variable required for openrouter provider")
            return 1
        provider = OpenRouterProvider(api_key=api_key)

    # Select model (defaults based on provider)
    model = args.model
    if model is None:
        model = "qwen2.5:7b-instruct" if args.provider == "ollama" else "qwen/qwen2.5-coder-7b-instruct"

    agent = LLMAgent(provider=provider, model=model, role=args.role)
    print(f"Using LLM agent: {args.provider}/{model} (role: {args.role})")
else:
    agent = HeuristicAgent()
```

**Important:** For batch simulation with LLM agent, the circuit breaker needs to be reset between combat runs. Update `run_batch_simulation` and/or the batch runner integration:
- Check if agent has `reset_circuit_breaker` method (duck typing)
- If so, the MonteCarloSimulator/BatchRunner should call it before each combat run
- The simplest approach: In `run.py`, wrap the batch runner call or pass the agent reset logic. OR: modify `run_combat` in simulator.py to call `agent.reset_circuit_breaker()` if the method exists at the start of each combat. Choose the least invasive approach -- checking `hasattr(agent, 'reset_circuit_breaker')` in `run_combat` before the combat loop is cleanest.

Update the examples in batch_args.py epilog to show LLM usage:
```
# With LLM agent (local Ollama)
python run.py --party fighter.md --enemies goblin goblin --agent llm --runs 100

# With LLM agent (cloud OpenRouter)
python run.py --party fighter.md --enemies goblin --agent llm --provider openrouter --runs 50

# With role archetype
python run.py --party fighter.md --enemies goblin --agent llm --role striker
```
  </action>
  <verify>Run `cd /Users/enrico/workspace/myobsidian/dnd-simulator && python run.py --help` -- shows --agent, --provider, --model, --role flags with correct help text. Then run `python run.py --party fighter.md --enemies goblin --agent heuristic` -- still works as before (heuristic mode). Then run `python -m pytest tests/ -v` -- all existing tests pass.</verify>
  <done>CLI accepts --agent llm --provider ollama/openrouter --model MODEL --role ROLE flags. Heuristic agent still works as default. LLM agent is instantiated with correct provider/model/role when --agent llm is specified. Circuit breaker resets between batch combat runs. Help text shows LLM usage examples.</done>
</task>

</tasks>

<verification>
1. `python run.py --help` shows --agent, --provider, --model, --role flags
2. `python run.py --party fighter.md --enemies goblin` still works (heuristic default)
3. LLMAgent instantiates with OllamaProvider or OpenRouterProvider
4. LLMAgent.choose_action returns AgentAction (falls back to heuristic if no Ollama running)
5. Circuit breaker opens after 3 consecutive failures
6. Circuit breaker resets between combat runs in batch mode
7. `python -m pytest tests/ -v` -- all tests pass
</verification>

<success_criteria>
- LLMAgent orchestrates: build_prompt -> provider.chat_completion -> parse_llm_output -> validate_move -> to_agent_action
- On any failure in the chain, HeuristicAgent.choose_action is called as fallback
- Circuit breaker disables LLM after 3 consecutive failures, resets between combats
- CLI --agent llm activates LLM agent with correct provider and model
- Existing heuristic mode is unaffected (no regressions)
- Running with --agent llm and Ollama not available gracefully falls back to heuristic (not crash)
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-tactical-agents/03-03-SUMMARY.md`
</output>
