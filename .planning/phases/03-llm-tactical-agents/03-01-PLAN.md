---
phase: 03-llm-tactical-agents
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/agents/llm_parser.py
  - src/agents/llm_validator.py
  - tests/test_llm_parser.py
  - tests/test_llm_validator.py
autonomous: true

must_haves:
  truths:
    - "LLM text output with <thinking> block + ACTION/TARGET/MOVEMENT/BONUS/REACTION keys is parsed into structured data"
    - "Malformed LLM output (missing tags, extra text, markdown wrapping) is handled gracefully"
    - "Illegal moves (out-of-range attack, dead target, movement exceeds speed) are rejected with error message"
    - "Valid moves pass validation and convert to AgentAction"
  artifacts:
    - path: "src/agents/llm_parser.py"
      provides: "LLM output text -> LLMResponse Pydantic model"
      exports: ["LLMResponse", "parse_llm_output"]
    - path: "src/agents/llm_validator.py"
      provides: "Move legality validation against CombatState"
      exports: ["validate_move", "to_agent_action"]
    - path: "tests/test_llm_parser.py"
      provides: "Parser tests covering clean output, malformed output, missing fields"
    - path: "tests/test_llm_validator.py"
      provides: "Validator tests covering legal moves, illegal moves, edge cases"
  key_links:
    - from: "src/agents/llm_parser.py"
      to: "pydantic"
      via: "LLMResponse BaseModel with field_validators"
      pattern: "class LLMResponse.*BaseModel"
    - from: "src/agents/llm_validator.py"
      to: "src/domain/distance.py"
      via: "distance_in_feet for range checking"
      pattern: "distance_in_feet"
    - from: "src/agents/llm_validator.py"
      to: "src/agents/base.py"
      via: "converts validated response to AgentAction"
      pattern: "AgentAction"
---

<objective>
Build and test the LLM output parser and move validator using TDD.

Purpose: The parser and validator are the critical safety layer between LLM output and the combat engine. If the parser fails to handle malformed output, or the validator lets illegal moves through, the simulation crashes or produces meaningless results. TDD ensures these components handle edge cases correctly before integration.

Output: Two tested modules -- `llm_parser.py` (text -> LLMResponse) and `llm_validator.py` (LLMResponse + CombatState -> AgentAction or error).
</objective>

<execution_context>
@/Users/enrico/.claude/get-shit-done/workflows/execute-plan.md
@/Users/enrico/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-tactical-agents/03-RESEARCH.md

@src/agents/base.py
@src/domain/creature.py
@src/domain/combat_state.py
@src/domain/distance.py
@.requirements/LLM System Prompt for Monster Agent.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: TDD - LLM Output Parser</name>
  <files>src/agents/llm_parser.py, tests/test_llm_parser.py</files>
  <action>
**RED phase:** Write tests first in `tests/test_llm_parser.py`.

Create a Pydantic model `LLMResponse` with fields: thinking (str), action (str), target (str|None), movement (str|None), bonus (str|None), reaction (str|None).

Test cases to write FIRST:

1. **Clean output parsing** - Full well-formed LLM response with all fields:
```
<thinking>Fighter is nearby at D4, 15ft away. I should close distance and attack with my scimitars.</thinking>
ACTION: Multiattack with Scimitar
TARGET: Fighter (D4)
MOVEMENT: from E5 to D5
BONUS: Hide
REACTION: Opportunity attack on anyone leaving reach
```
Expected: thinking extracted, action="Multiattack with Scimitar", target="Fighter (D4)", movement="from E5 to D5", bonus="Hide", reaction set.

2. **Missing optional fields** - Response with only ACTION (no TARGET, MOVEMENT, BONUS, REACTION):
```
<thinking>No enemies in range, I should dodge.</thinking>
ACTION: Dodge
```
Expected: action="Dodge", target=None, movement=None, bonus=None, reaction=None.

3. **Malformed thinking tag** - Missing closing tag, extra whitespace:
```
<thinking>I need to attack the wizard.

ACTION: Attack with Longsword
TARGET: Wizard (C3)
```
Expected: thinking extracted (content after opening tag), action parsed correctly.

4. **Extra preamble/markdown wrapping** - LLM adds "Here's my decision:" before format, or wraps in ```xml blocks:
```
Here is my tactical decision:

<thinking>Attack the weakest enemy.</thinking>
ACTION: Attack with Bite
TARGET: Rogue (B2)
MOVEMENT: stay
BONUS: None
REACTION: None
```
Expected: Preamble ignored, all fields parsed.

5. **Case insensitivity** - Mixed case keys like "action:" or "Action:":
Expected: Keys parsed regardless of case.

6. **"None"/"stay"/"N/A" values** - Various null representations:
Expected: Converted to None.

7. **No thinking tag at all** - LLM skips thinking:
Expected: thinking="" (empty string), other fields still parsed.

8. **Completely garbage output** - Random text with no format:
Expected: Raises `ValueError` or returns with defaults (action="dodge").

**GREEN phase:** Implement `src/agents/llm_parser.py`:

- `LLMResponse` Pydantic BaseModel with the 6 fields above
- `parse_llm_output(text: str) -> LLMResponse` function that:
  1. Extracts `<thinking>` content with regex (handles missing close tag)
  2. Extracts key-value pairs (ACTION, TARGET, MOVEMENT, BONUS, REACTION) with regex per line
  3. Treats "none", "stay", "n/a", "-", "" as None for optional fields
  4. Returns LLMResponse with validated fields
  5. On total parse failure (no ACTION found), raises ValueError

Use `re.search(r'<thinking>\s*(.*?)\s*</thinking>', text, re.DOTALL | re.IGNORECASE)` for thinking.
Use `re.search(rf'^{key}:\s*(.+?)(?:\n|$)', text, re.MULTILINE | re.IGNORECASE)` for key-value.

Do NOT add Pydantic field validators that reject unknown action names here -- that is the validator's job. The parser just extracts text faithfully.
  </action>
  <verify>Run `cd /Users/enrico/workspace/myobsidian/dnd-simulator && python -m pytest tests/test_llm_parser.py -v` -- all tests pass.</verify>
  <done>parse_llm_output correctly extracts structured data from clean, malformed, and garbage LLM output. 8+ test cases pass.</done>
</task>

<task type="auto">
  <name>Task 2: TDD - Move Legality Validator</name>
  <files>src/agents/llm_validator.py, tests/test_llm_validator.py</files>
  <action>
**RED phase:** Write tests first in `tests/test_llm_validator.py`.

Two functions to test:
- `validate_move(response: LLMResponse, state: CombatState, creature_id: str) -> tuple[bool, str]`
- `to_agent_action(response: LLMResponse, state: CombatState, creature_id: str) -> AgentAction`

Create test helper to build minimal CombatState with 2 creatures (attacker at E5, target at D4, both alive, speed 30).

Test cases to write FIRST:

1. **Valid melee attack in range** - Creature at E5, target at D4 (5ft), attack with Scimitar:
Expected: (True, "")

2. **Target does not exist** - Target ID "wizard_0" not in state.creatures:
Expected: (False, "Target ... does not exist")

3. **Target is dead** - Target at 0 HP:
Expected: (False, "Target ... is already dead")

4. **Movement exceeds speed** - Movement from E5 to Z20 (way beyond 30ft speed):
Expected: (False, "Movement ... exceeds speed ...")

5. **Attack out of range** - Melee attack (reach 5ft) on target 30ft away, no movement specified:
Expected: (False, "... out of range ...")

6. **Attack with movement into range** - Target 30ft away, creature moves to adjacent square, then attacks:
Expected: (True, "")

7. **Valid dodge action** - No target needed:
Expected: (True, "")

8. **Attack action missing target** - action="Attack with Scimitar" but target=None:
Expected: (False, "Attack requires a target")

9. **to_agent_action conversion** - Valid parsed response converts to AgentAction with correct action_type, attack_name, target_id, move_to fields.

**Creature/action matching logic:** The validator needs to resolve LLM's free-text action name (e.g., "Multiattack with Scimitar") to the creature's actual Action objects. Use fuzzy matching:
- If action text contains "multiattack", find the Multiattack action
- If action text contains an attack name (e.g., "Scimitar", "Bite"), find matching action
- If action text is "dodge"/"Dodge", return action_type="dodge"

**Target matching logic:** The LLM says "Fighter (D4)" but the state uses creature_ids like "fighter_0". The validator must:
- Strip parenthetical position info: "Fighter (D4)" -> "Fighter"
- Match against creature names (case-insensitive) to find creature_id
- If multiple matches, prefer the one closest to the creature

**Movement parsing:** The LLM says "from E5 to D5" or just "D5" or "stay". Extract the destination grid position.

**GREEN phase:** Implement `src/agents/llm_validator.py`:

- `validate_move(response, state, creature_id) -> tuple[bool, str]`:
  1. Parse movement destination from response.movement (handle "from X to Y", bare position, "stay"/None)
  2. Validate movement distance <= creature.speed using distance_in_feet
  3. Resolve target: match response.target text to creature_id in state
  4. Check target exists and is alive (current_hp > 0)
  5. Resolve action: match response.action to creature's Action objects
  6. If attack action: check range from final position (after movement) to target using distance_in_feet
  7. Return (True, "") or (False, error_message)

- `to_agent_action(response, state, creature_id) -> AgentAction`:
  1. Call validate_move first (raise ValueError if invalid)
  2. Determine action_type: "dodge", "move", "attack", "move_and_attack"
  3. Map resolved action name and target_id
  4. Return AgentAction

Import from existing modules: `from src.domain.distance import distance_in_feet`, `from src.agents.base import AgentAction`.
  </action>
  <verify>Run `cd /Users/enrico/workspace/myobsidian/dnd-simulator && python -m pytest tests/test_llm_validator.py -v` -- all tests pass. Then run `python -m pytest tests/ -v` to confirm no regressions.</verify>
  <done>validate_move correctly rejects illegal moves with descriptive error messages. to_agent_action converts valid LLM responses to AgentAction. 9+ test cases pass. All existing tests still pass.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_llm_parser.py -v` -- all parser tests pass
2. `python -m pytest tests/test_llm_validator.py -v` -- all validator tests pass
3. `python -m pytest tests/ -v` -- no regressions in existing tests
4. Parser handles malformed output without crashing
5. Validator catches all illegal move types (dead target, out of range, exceeds speed)
</verification>

<success_criteria>
- LLMResponse Pydantic model parses clean and malformed LLM output
- parse_llm_output handles 8+ edge cases (missing tags, extra preamble, case insensitive, garbage)
- validate_move rejects illegal moves with descriptive error messages
- to_agent_action produces correct AgentAction from validated LLM response
- Target name resolution works ("Fighter (D4)" -> "fighter_0")
- Movement parsing works ("from E5 to D5" -> "D5")
- All tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-tactical-agents/03-01-SUMMARY.md`
</output>
