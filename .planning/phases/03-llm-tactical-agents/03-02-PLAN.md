---
phase: 03-llm-tactical-agents
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/agents/llm_providers.py
  - src/agents/llm_prompt.py
autonomous: true

must_haves:
  truths:
    - "Ollama provider calls local LLM and returns text response synchronously"
    - "OpenRouter provider calls cloud LLM via OpenAI SDK and returns text response synchronously"
    - "Prompt builder constructs combat state prompt matching user's specified format"
    - "Creature role archetypes (tank/striker/controller/support) modify system prompt behavior"
  artifacts:
    - path: "src/agents/llm_providers.py"
      provides: "OllamaProvider, OpenRouterProvider with synchronous chat_completion"
      exports: ["LLMProvider", "OllamaProvider", "OpenRouterProvider"]
    - path: "src/agents/llm_prompt.py"
      provides: "Combat state -> LLM prompt messages"
      exports: ["build_prompt", "ROLE_PROMPTS"]
    - path: "pyproject.toml"
      provides: "ollama and openai dependencies added"
      contains: "ollama"
  key_links:
    - from: "src/agents/llm_providers.py"
      to: "ollama"
      via: "ollama.Client sync API"
      pattern: "ollama.*Client"
    - from: "src/agents/llm_providers.py"
      to: "openai"
      via: "openai.OpenAI with OpenRouter base_url"
      pattern: "OpenAI.*openrouter"
    - from: "src/agents/llm_prompt.py"
      to: "src/domain/combat_state.py"
      via: "reads CombatState to build prompt"
      pattern: "state\\.creatures"
    - from: "src/agents/llm_prompt.py"
      to: "src/domain/distance.py"
      via: "calculates distances for tactical notes"
      pattern: "distance_in_feet"
---

<objective>
Create LLM provider abstraction (Ollama + OpenRouter) and combat prompt builder with role archetypes.

Purpose: These are the two input/output interfaces for LLM integration. Providers handle the "calling the LLM" concern. The prompt builder handles the "what to tell the LLM" concern. Both are needed before the LLMAgent can orchestrate them.

Output: `llm_providers.py` (sync Ollama + OpenRouter clients), `llm_prompt.py` (combat state -> formatted messages with role-based system prompts), updated `pyproject.toml` with new dependencies.
</objective>

<execution_context>
@/Users/enrico/.claude/get-shit-done/workflows/execute-plan.md
@/Users/enrico/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-tactical-agents/03-RESEARCH.md

@src/agents/base.py
@src/domain/creature.py
@src/domain/combat_state.py
@src/domain/distance.py
@.requirements/LLM System Prompt for Monster Agent.md
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add LLM dependencies and create provider abstraction</name>
  <files>pyproject.toml, src/agents/llm_providers.py</files>
  <action>
**Step 1: Update pyproject.toml** -- Add `ollama>=0.4.7` and `openai>=1.0` to the dependencies list. Also add `tenacity>=8.3` (needed by Plan 03 for retry logic, but install now so the environment is ready). Do NOT add `instructor` -- we are using regex+Pydantic per research recommendations.

**Step 2: Install dependencies** -- Run `cd /Users/enrico/workspace/myobsidian/dnd-simulator && pip install -e ".[dev]"` to install new deps.

**Step 3: Create `src/agents/llm_providers.py`** with:

```python
from abc import ABC, abstractmethod

class LLMProvider(ABC):
    @abstractmethod
    def chat_completion(self, messages: list[dict], model: str, temperature: float = 0.7, max_tokens: int = 300) -> str:
        """Send messages to LLM and return text response."""
        ...
```

**OllamaProvider** (synchronous):
- Constructor takes `host: str = "http://localhost:11434"`
- `chat_completion` uses `ollama.Client(host=self.host).chat(model=model, messages=messages, options={"temperature": temperature, "num_predict": max_tokens})`
- Returns `response['message']['content']`
- Use the synchronous `ollama.Client`, NOT `AsyncClient` (the combat simulator is synchronous -- see architectural context in planning)

**OpenRouterProvider** (synchronous):
- Constructor takes `api_key: str`, optional `base_url: str = "https://openrouter.ai/api/v1"`
- `chat_completion` uses `openai.OpenAI(base_url=self.base_url, api_key=self.api_key).chat.completions.create(model=model, messages=messages, temperature=temperature, max_tokens=max_tokens)`
- Returns `response.choices[0].message.content`
- Use the synchronous `openai.OpenAI`, NOT `AsyncOpenAI` (same reason)

Keep both providers simple -- no retry logic here. Retry is handled at the LLMAgent level (Plan 03).

Both providers should handle their SDK's exceptions and let them propagate (the agent catches them).
  </action>
  <verify>Run `cd /Users/enrico/workspace/myobsidian/dnd-simulator && python -c "from src.agents.llm_providers import LLMProvider, OllamaProvider, OpenRouterProvider; print('Providers imported OK')"` -- imports succeed without error.</verify>
  <done>pyproject.toml has ollama, openai, tenacity dependencies. OllamaProvider and OpenRouterProvider both implement LLMProvider with synchronous chat_completion. Both can be instantiated without errors.</done>
</task>

<task type="auto">
  <name>Task 2: Create prompt builder with role archetypes</name>
  <files>src/agents/llm_prompt.py</files>
  <action>
Create `src/agents/llm_prompt.py` that builds LLM prompt messages from combat state.

**Role archetype system** (AGENT-06): Define a dict `ROLE_PROMPTS` mapping role strings to behavioral instructions:

```python
ROLE_PROMPTS = {
    "tank": "You are a TANK. Prioritize protecting weaker allies. Position yourself between enemies and your team. Use defensive abilities. Draw attacks to yourself.",
    "striker": "You are a STRIKER. Prioritize maximum damage on high-value targets. Focus fire on wounded enemies. Use positioning for advantage.",
    "controller": "You are a CONTROLLER. Prioritize area denial and crowd control. Target clusters of enemies. Limit enemy movement and options.",
    "support": "You are a SUPPORT. Prioritize keeping allies alive and buffed. Stay at range. Use healing and buff abilities before attacking.",
    "default": "You are a tactical monster agent. Maximize your side's chance of victory."
}
```

**build_prompt function:**

`def build_prompt(state: CombatState, creature_id: str, role: str = "default") -> list[dict]:`

Returns `[{"role": "system", "content": system_prompt}, {"role": "user", "content": turn_prompt}]`.

**System prompt** (built from user's format in `.requirements/LLM System Prompt for Monster Agent.md`):
- Start with the role prompt from ROLE_PROMPTS
- Add fixed rules (use only stat block actions, respect range/reach, never exceed movement speed, do not invent abilities)
- Add the mandatory output format instructions (thinking block + ACTION/TARGET/MOVEMENT/BONUS/REACTION keys)

**User/turn prompt** (the actual combat state):
Build this from the CombatState following the user's specified input format:

```
Round {state.round} | Your turn: {creature.name} (current position: {creature.position})

=== Combat State ===
Enemy team:
{for each enemy creature: "- {name} HP {current_hp}/{hp_max} AC {ac} Pos: {position}"}

Your team:
{for each ally + self: "- {You/name} HP {current_hp}/{hp_max} AC {ac} Pos: {position}"}

Your stats this turn:
Speed: {creature.speed}ft  Remaining movement: {creature.speed}ft
Available actions:
{for each action in creature.actions: "- {action.name}: {describe attacks with bonus and damage}"}

Tactical notes:
{for each enemy: "- Distance to {name}: {distance_in_feet}ft"}
```

**Important implementation details:**
- Use `distance_in_feet` from `src.domain.distance` for tactical notes
- Mark the current creature with "You" prefix in team listing
- Only show creatures with current_hp > 0 (skip dead creatures)
- List enemies as creatures on different team, allies as same team
- For each action's attacks, show: "{attack.name} +{attack_bonus}, {damage.dice} {damage.damage_type}, reach/range {range_feet}ft"
- Keep prompt concise to stay within 2K-4K token budget for 7B models (research pitfall #6). Only show essential combat info. No full stat blocks.
  </action>
  <verify>Run `cd /Users/enrico/workspace/myobsidian/dnd-simulator && python -c "
from src.agents.llm_prompt import build_prompt, ROLE_PROMPTS
from src.domain.creature import Creature, Action, Attack, DamageRoll, AbilityScores
from src.domain.combat_state import CombatState

# Create test creatures
fighter = Creature(name='Fighter', ac=18, hp_max=68, current_hp=38, speed=30, team='party', position='D4', creature_id='fighter_0', actions=[Action(name='Longsword', attacks=[Attack(name='Longsword', attack_bonus=7, damage=DamageRoll(dice='1d8+4', damage_type='slashing'), reach=5)])])
goblin = Creature(name='Goblin Boss', ac=17, hp_max=21, current_hp=12, speed=30, team='enemy', position='E5', creature_id='goblin_boss_0', actions=[Action(name='Multiattack', attacks=[Attack(name='Scimitar', attack_bonus=5, damage=DamageRoll(dice='1d6+3', damage_type='slashing'), reach=5), Attack(name='Scimitar', attack_bonus=5, damage=DamageRoll(dice='1d6+3', damage_type='slashing'), reach=5)])])
creatures = {'fighter_0': fighter, 'goblin_boss_0': goblin}
state = CombatState(creatures=creatures, initiative_order=['goblin_boss_0', 'fighter_0'], round=3)

messages = build_prompt(state, 'goblin_boss_0', role='striker')
print('System prompt length:', len(messages[0]['content']))
print('User prompt length:', len(messages[1]['content']))
print()
print('--- SYSTEM ---')
print(messages[0]['content'][:200])
print()
print('--- USER ---')
print(messages[1]['content'])
"` -- produces well-formatted prompt with role prefix, combat state, and tactical notes.</verify>
  <done>build_prompt produces [system, user] message pair. System prompt includes role archetype + rules + format instructions. User prompt shows combat state with HP/AC/position, available actions with attack details, and distance-based tactical notes. ROLE_PROMPTS dict has tank/striker/controller/support/default entries.</done>
</task>

</tasks>

<verification>
1. `pip install -e ".[dev]"` succeeds with ollama, openai, tenacity installed
2. Provider classes import and instantiate without error
3. Prompt builder produces correctly formatted messages matching user's specified format
4. Role archetypes modify system prompt (STRIKER vs TANK vs default produce different instructions)
5. Tactical notes show correct distances between creatures
6. All existing tests still pass: `python -m pytest tests/ -v`
</verification>

<success_criteria>
- OllamaProvider and OpenRouterProvider implement synchronous LLMProvider interface
- build_prompt generates combat state in the user's specified format
- Role archetypes (tank/striker/controller/support) produce distinct system prompts
- Prompt stays concise (system + user < ~2000 tokens for 2v2 combat)
- Dependencies installed and importable
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-tactical-agents/03-02-SUMMARY.md`
</output>
